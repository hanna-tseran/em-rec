{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "# from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# from sklearn import svm\n",
    "import cv2\n",
    "\n",
    "import av\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "WORKING_FOLDER = \"/home/eola/research\"\n",
    "DB_FOLDER = \"{}/AFEW 5.0\". format(WORKING_FOLDER)\n",
    "FRAMES_FOLDER = \"{}/Frames\". format(WORKING_FOLDER)\n",
    "DB_TRAIN = \"{}/Train\". format(DB_FOLDER)\n",
    "DB_VAL = \"{}/Val\". format(DB_FOLDER)\n",
    "FRAMES_VAL =  \"{}/Val\". format(FRAMES_FOLDER)\n",
    "\n",
    "DICTIONARY_SIZE = 32 # cluster count\n",
    "\n",
    "class_to_name = {0: 'Angry', 1: 'Sad', 2: 'Neutral', 3: 'Disgust', 4: 'Surprise', 5: 'Fear', 6: 'Happy'}\n",
    "name_to_class = {'Angry': 0, 'Sad': 1, 'Neutral': 2, 'Disgust': 3, 'Surprise': 4, 'Fear': 5, 'Happy': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_videos_dict():\n",
    "    videos = {} #emotion_name: [] of videos names\n",
    "    for root, dirs, files in os.walk(DB_TRAIN):\n",
    "        # for all emotion folders\n",
    "        for dirname in dirs:\n",
    "            # put videos names in current emotion folder\n",
    "            videos[dirname] = []\n",
    "            for r, d, f in os.walk(\"{}/{}\".format(DB_TRAIN, dirname)):\n",
    "                videos[dirname].extend(f)\n",
    "                break    \n",
    "        # process root only\n",
    "        break\n",
    "\n",
    "#     for emotion, videos_list in videos.iteritems():\n",
    "#         print \"{}: {}\".format(emotion, len(videos_list))\n",
    "        \n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_frames():\n",
    "    if os.path.exists(FRAMES_FOLDER):\n",
    "        shutil.rmtree(FRAMES_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_frames():\n",
    "    print \"extract_frames\"\n",
    "    \n",
    "    if os.path.exists(FRAMES_FOLDER):\n",
    "        print \"Frames have already been extracted to {}\".format(FRAMES_FOLDER)\n",
    "        return\n",
    "       \n",
    "    videos = get_videos_dict()\n",
    "    os.mkdir(FRAMES_FOLDER)\n",
    "    for emotion_name, videos_list in videos.iteritems():\n",
    "        print \"{}: {}\".format(emotion_name, len(videos_list))\n",
    "        current_db_folder = \"{}/{}\".format(DB_TRAIN, emotion_name)\n",
    "        current_frames_folder = \"{}/{}\".format(FRAMES_FOLDER, emotion_name)\n",
    "        os.mkdir(current_frames_folder)\n",
    "        video_count = 0\n",
    "        for video in videos_list:\n",
    "            video_frames_path = \"{}/{}\".format(current_frames_folder, video)\n",
    "            os.mkdir(video_frames_path)\n",
    "            container = av.open(\"{}/{}\".format(current_db_folder, video))\n",
    "            \n",
    "            # extract frames amd save them as jpg files\n",
    "            count = 0\n",
    "            frames = []\n",
    "            for packet in container.demux():\n",
    "                for frame in packet.decode():\n",
    "                    if(type(frame) == av.VideoFrame):\n",
    "                        frame_path = \"{}/{}.jpg\".format(video_frames_path, count)\n",
    "                        frame.to_image().save(frame_path)\n",
    "#                         img = mpimg.imread(frame_path)\n",
    "#                         plt.imshow(img)\n",
    "#                         plt.show\n",
    "                        count += 1\n",
    "    \n",
    "            video_count += 1\n",
    "            if not video_count % 10:\n",
    "                print video_count,\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_frames\n",
      "Frames have already been extracted to /home/eola/research/Frames\n"
     ]
    }
   ],
   "source": [
    "# delete_frames()\n",
    "extract_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_frames_dict():\n",
    "    frames = {} # {emotion_name: {video_name: [] of frames names}}\n",
    "    for root, dirs, files in os.walk(FRAMES_FOLDER):\n",
    "        # for all emotion folders\n",
    "        for dirname in dirs:\n",
    "            # put videos names in current emotion folder\n",
    "            frames[dirname] = {}\n",
    "            for vr, videodirs, vf in os.walk(\"{}/{}\".format(FRAMES_FOLDER, dirname)):\n",
    "                for videoname in videodirs:\n",
    "                    frames[dirname][videoname] = []\n",
    "                    for r, d, f in os.walk(\"{}/{}/{}\".format(FRAMES_FOLDER, dirname, videoname)):\n",
    "                        frames[dirname][videoname] .extend(f)\n",
    "                        break                \n",
    "                break    \n",
    "        # process root only\n",
    "        break\n",
    "\n",
    "#     for emotion, videos_list in videos.iteritems():\n",
    "#         print \"{}: {}\".format(emotion, len(videos_list))\n",
    "        \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val\n",
      "0\n",
      "Angry\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-3f1dc01f2712>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}/{}/{}.jpg\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFRAMES_FOLDER\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memotion_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCV_LOAD_IMAGE_GRAYSCALE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mkp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdsc\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msift\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;31m#             print \"dsc size: {}\".format(len(dsc))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "frames_dict = get_frames_dict()\n",
    "# print frames\n",
    "\n",
    "# print \"number of emotions: {}\".format(len(frames_dict.keys()))\n",
    "# bow = cv2.BOWKMeansTrainer(len(frames_dict.keys()))\n",
    "bow = cv2.BOWKMeansTrainer(DICTIONARY_SIZE)\n",
    "\n",
    "sift = cv2.SIFT()\n",
    "descriptors = []\n",
    "for emotion_name, videos_dict in frames_dict.iteritems():\n",
    "    print emotion_name\n",
    "    for video_name, frames in videos_dict.iteritems():\n",
    "#         print \"{}: {} - {}\".format(emotion_name, video_name, len(frames))\n",
    "        for i in xrange(0, len(frames), 25):\n",
    "#             print i\n",
    "            frame = cv2.imread(\"{}/{}/{}/{}.jpg\".format(FRAMES_FOLDER, emotion_name, video_name, i))\n",
    "            gray = cv2.cvtColor(frame, cv2.CV_LOAD_IMAGE_GRAYSCALE)\n",
    "            kp, dsc= sift.detectAndCompute(gray, None)\n",
    "#             print \"dsc size: {}\".format(len(dsc))\n",
    "            if len(kp):\n",
    "                bow.add(dsc)\n",
    "            else:\n",
    "                print \"no descriptors to add, kp len: {}\".format(len(kp))\n",
    "    print len(bow.getDescriptors())\n",
    "            \n",
    "dictionary = bow.cluster()\n",
    "print len(dictionary)\n",
    "\n",
    "\n",
    "#             print \"number of descriptors: \" + str(len(descriptors))\n",
    "#             print descriptors\n",
    "#         for i in range(,frames[emotion][])\n",
    "# for path in training_paths:\n",
    "#     image = cv2.imread(path)\n",
    "#     print path\n",
    "#     gray = cv2.cvtColor(image, cv2.CV_LOAD_IMAGE_GRAYSCALE)\n",
    "#     kp, dsc= sift.detectAndCompute(gray, None)\n",
    "#     descriptors.append(dsc)\n",
    "\n",
    "# des = np.array(descriptors)\n",
    "\n",
    "# k=5\n",
    "# bow = cv2.BOWKMeansTrainer(k)\n",
    "# bow.cluster(des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_dict(dictionary):\n",
    "    output = open('dict.pkl', 'wb')\n",
    "    pickle.dump(dictionary, output)\n",
    "    output.close()\n",
    "    \n",
    "def load_dict():\n",
    "    pkl_file = open('dict.pkl', 'rb')\n",
    "    dictionary = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Val\n",
      "0\n",
      "Angry\n",
      "346\n",
      "Sad\n",
      "343\n",
      "Neutral\n",
      "404\n",
      "Disgust\n",
      "238\n",
      "Surprise\n",
      "207\n",
      "Fear\n",
      "no descriptors, kp len: 0\n",
      "207\n",
      "Happy\n",
      "407\n"
     ]
    }
   ],
   "source": [
    "bow_dict = load_dict()\n",
    "print len(bow_dict)\n",
    "\n",
    "frames_dict = get_frames_dict()\n",
    "\n",
    "sift = cv2.SIFT()\n",
    "bow_ext = cv2.BOWImgDescriptorExtractor(cv2.DescriptorExtractor_create(\"SIFT\"), cv2.BFMatcher(cv2.NORM_L2))\n",
    "bow_ext.setVocabulary(bow_dict)\n",
    "histograms = {}\n",
    "for emotion_name, videos_dict in frames_dict.iteritems():\n",
    "    print emotion_name\n",
    "    histograms[emotion_name] = []\n",
    "    for video_name, frames in videos_dict.iteritems():\n",
    "        for i in xrange(0, len(frames), 25):\n",
    "            frame = cv2.imread(\"{}/{}/{}/{}.jpg\".format(FRAMES_FOLDER, emotion_name, video_name, i))\n",
    "            gray = cv2.cvtColor(frame, cv2.CV_LOAD_IMAGE_GRAYSCALE)\n",
    "            kp, dsc= sift.detectAndCompute(gray, None)\n",
    "            if len(kp):\n",
    "                histogram = bow_ext.compute(gray, kp)[0]\n",
    "                histograms[emotion_name].append(histogram)\n",
    "            else:\n",
    "                print \"no descriptors, kp len: {}\".format(len(kp))\n",
    "            \n",
    "    print len(histograms[emotion_name])\n",
    "#             print len(histogram)\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Val', 'Angry', 'Sad', 'Neutral', 'Disgust', 'Surprise', 'Fear', 'Happy']\n"
     ]
    }
   ],
   "source": [
    "print histograms.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Angry', 1: 'Sad', 2: 'Neutral', 3: 'Disgust', 4: 'Surprise', 5: 'Fear', 6: 'Happy'}\n",
      "{'Angry': 0, 'Sad': 1, 'Neutral': 2, 'Disgust': 3, 'Surprise': 4, 'Fear': 5, 'Happy': 6}\n",
      "2152\n",
      "2152\n"
     ]
    }
   ],
   "source": [
    "# i = 0\n",
    "# for emotion_name in frames_dict.keys():\n",
    "#     class_to_name[i] = emotion_name\n",
    "#     name_to_class[emotion_name] = i\n",
    "#     i += 1\n",
    "print class_to_name\n",
    "print name_to_class\n",
    "\n",
    "# data = [[name_to_class[emotion_name]] * len(histogram[emotion_name]) for emotion_name in histograms.keys()]\n",
    "# data = [[name_to_class[emotion_name]] * 5 for emotion_namse in histograms.keys()]\n",
    "# join(data)\n",
    "target = [name_to_class[emotion_name] for emotion_name in histograms.keys() for i in xrange(len(histograms[emotion_name]))]\n",
    "print len(target)\n",
    "data = [hist for hists in histograms.values() for hist in hists]\n",
    "print len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65799256505576209"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='rbf', C=50, gamma=10) # gamma=0.001, C=100\n",
    "clf.fit(data, target)\n",
    "clf.score(data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_frames\n",
      "Frames have already been extracted to /home/eola/research/Frames/Val\n",
      "183\n",
      "1083\n",
      "1083\n"
     ]
    }
   ],
   "source": [
    "def get_videos_dict(path):\n",
    "    videos = {} #emotion_name: [] of videos names\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # for all emotion folders\n",
    "        for dirname in dirs:\n",
    "            # put videos names in current emotion folder\n",
    "            videos[dirname] = []\n",
    "            for r, d, f in os.walk(\"{}/{}\".format(path, dirname)):\n",
    "                videos[dirname].extend(f)\n",
    "                break    \n",
    "        # process root only\n",
    "        break\n",
    "\n",
    "#     for emotion, videos_list in videos.iteritems():\n",
    "#         print \"{}: {}\".format(emotion, len(videos_list))\n",
    "        \n",
    "    return videos\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "def extract_frames(db_path, frames_path):\n",
    "    print \"extract_frames\"\n",
    "    \n",
    "    if os.path.exists(frames_path):\n",
    "        print \"Frames have already been extracted to {}\".format(frames_path)\n",
    "        return\n",
    "       \n",
    "    videos = get_videos_dict(db_path)\n",
    "    os.mkdir(frames_path)\n",
    "    for emotion_name, videos_list in videos.iteritems():\n",
    "        print \"{}: {}\".format(emotion_name, len(videos_list))\n",
    "        current_db_folder = \"{}/{}\".format(db_path, emotion_name)\n",
    "        current_frames_folder = \"{}/{}\".format(frames_path, emotion_name)\n",
    "        os.mkdir(current_frames_folder)\n",
    "        video_count = 0\n",
    "        for video in videos_list:\n",
    "            video_frames_path = \"{}/{}\".format(current_frames_folder, video)\n",
    "            os.mkdir(video_frames_path)\n",
    "            container = av.open(\"{}/{}\".format(current_db_folder, video))\n",
    "            \n",
    "            # extract frames amd save them as jpg files\n",
    "            count = 0\n",
    "            frames = []\n",
    "            for packet in container.demux():\n",
    "                for frame in packet.decode():\n",
    "                    if(type(frame) == av.VideoFrame):\n",
    "                        frame_path = \"{}/{}.jpg\".format(video_frames_path, count)\n",
    "                        frame.to_image().save(frame_path)\n",
    "#                         img = mpimg.imread(frame_path)\n",
    "#                         plt.imshow(img)\n",
    "#                         plt.show\n",
    "                        count += 1\n",
    "    \n",
    "            video_count += 1\n",
    "            if not video_count % 10:\n",
    "                print video_count,\n",
    "        print\n",
    "        \n",
    "###############################################################################################\n",
    "\n",
    "def get_frames_dict(path):\n",
    "    frames = {} # {emotion_name: {video_name: [] of frames names}}\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # for all emotion folders\n",
    "        for dirname in dirs:\n",
    "            # put videos names in current emotion folder\n",
    "            frames[dirname] = {}\n",
    "            for vr, videodirs, vf in os.walk(\"{}/{}\".format(path, dirname)):\n",
    "                for videoname in videodirs:\n",
    "                    frames[dirname][videoname] = []\n",
    "                    for r, d, f in os.walk(\"{}/{}/{}\".format(path, dirname, videoname)):\n",
    "                        frames[dirname][videoname] .extend(f)\n",
    "                        break                \n",
    "                break    \n",
    "        # process root only\n",
    "        break\n",
    "\n",
    "#     for emotion, videos_list in videos.iteritems():\n",
    "#         print \"{}: {}\".format(emotion, len(videos_list))\n",
    "        \n",
    "    return frames\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "def get_histograms(frames_path):\n",
    "    bow_dict = load_dict()\n",
    "    print len(bow_dict)\n",
    "\n",
    "    frames_dict = get_frames_dict(frames_path)\n",
    "\n",
    "    sift = cv2.SIFT()\n",
    "    bow_ext = cv2.BOWImgDescriptorExtractor(cv2.DescriptorExtractor_create(\"SIFT\"), cv2.BFMatcher(cv2.NORM_L2))\n",
    "    bow_ext.setVocabulary(bow_dict)\n",
    "    histograms = {}\n",
    "    for emotion_name, videos_dict in frames_dict.iteritems():\n",
    "        print emotion_name\n",
    "        histograms[emotion_name] = []\n",
    "        for video_name, frames in videos_dict.iteritems():\n",
    "            for i in xrange(0, len(frames), 25):\n",
    "                frame = cv2.imread(\"{}/{}/{}/{}.jpg\".format(frames_path, emotion_name, video_name, i))\n",
    "                gray = cv2.cvtColor(frame, cv2.CV_LOAD_IMAGE_GRAYSCALE)\n",
    "                kp, dsc= sift.detectAndCompute(gray, None)\n",
    "                if len(kp):\n",
    "                    histogram = bow_ext.compute(gray, kp)[0]\n",
    "                    histograms[emotion_name].append(histogram)\n",
    "                else:\n",
    "                    print \"no descriptors, kp len: {}\".format(len(kp))\n",
    "\n",
    "        print len(histograms[emotion_name])\n",
    "    return histograms\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "def split_hists(histograms):\n",
    "    data = [hist for hists in histograms.values() for hist in hists]\n",
    "    target = [name_to_class[emotion_name] for emotion_name in histograms.keys() for i in xrange(len(histograms[emotion_name]))]\n",
    "    return data, target\n",
    "    \n",
    "###############################################################################################\n",
    "\n",
    "# videos = get_videos_dict(DB_VAL)\n",
    "extract_frames(DB_VAL, FRAMES_VAL)\n",
    "# frames_hists = get_histograms(FRAMES_VAL)\n",
    "print len(frames_hists['Angry'])\n",
    "\n",
    "data_val, target_val = split_hists(frames_hists)\n",
    "print len(target_val)\n",
    "print len(data_val)\n",
    "# clf.score(data_val, target_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search for optimal SVM parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning SVM parameters\n",
      "best params: {'kernel': 'rbf', 'C': 5, 'gamma': 250}\n",
      "all results:\n",
      "0.212 (+/- 0.033) for {'kernel': 'rbf', 'C': 1, 'gamma': 225}\n",
      "0.211 (+/- 0.031) for {'kernel': 'rbf', 'C': 1, 'gamma': 250}\n",
      "0.212 (+/- 0.032) for {'kernel': 'rbf', 'C': 1, 'gamma': 275}\n",
      "0.216 (+/- 0.040) for {'kernel': 'rbf', 'C': 1, 'gamma': 300}\n",
      "0.220 (+/- 0.034) for {'kernel': 'rbf', 'C': 2, 'gamma': 225}\n",
      "0.218 (+/- 0.034) for {'kernel': 'rbf', 'C': 2, 'gamma': 250}\n",
      "0.216 (+/- 0.037) for {'kernel': 'rbf', 'C': 2, 'gamma': 275}\n",
      "0.218 (+/- 0.041) for {'kernel': 'rbf', 'C': 2, 'gamma': 300}\n",
      "0.222 (+/- 0.061) for {'kernel': 'rbf', 'C': 5, 'gamma': 225}\n",
      "0.224 (+/- 0.044) for {'kernel': 'rbf', 'C': 5, 'gamma': 250}\n",
      "0.224 (+/- 0.044) for {'kernel': 'rbf', 'C': 5, 'gamma': 275}\n",
      "0.224 (+/- 0.042) for {'kernel': 'rbf', 'C': 5, 'gamma': 300}\n",
      "0.222 (+/- 0.060) for {'kernel': 'rbf', 'C': 7, 'gamma': 225}\n",
      "0.223 (+/- 0.045) for {'kernel': 'rbf', 'C': 7, 'gamma': 250}\n",
      "0.224 (+/- 0.043) for {'kernel': 'rbf', 'C': 7, 'gamma': 275}\n",
      "0.223 (+/- 0.040) for {'kernel': 'rbf', 'C': 7, 'gamma': 300}\n",
      "0.222 (+/- 0.053) for {'kernel': 'rbf', 'C': 10, 'gamma': 225}\n",
      "0.223 (+/- 0.043) for {'kernel': 'rbf', 'C': 10, 'gamma': 250}\n",
      "0.224 (+/- 0.043) for {'kernel': 'rbf', 'C': 10, 'gamma': 275}\n",
      "0.223 (+/- 0.040) for {'kernel': 'rbf', 'C': 10, 'gamma': 300}\n",
      "0.223 (+/- 0.053) for {'kernel': 'rbf', 'C': 13, 'gamma': 225}\n",
      "0.223 (+/- 0.043) for {'kernel': 'rbf', 'C': 13, 'gamma': 250}\n",
      "0.224 (+/- 0.043) for {'kernel': 'rbf', 'C': 13, 'gamma': 275}\n",
      "0.223 (+/- 0.040) for {'kernel': 'rbf', 'C': 13, 'gamma': 300}\n"
     ]
    }
   ],
   "source": [
    "# params = [{'kernel': ['rbf'], 'gamma': [0.001, 0.01, 0.1, 1, 2, 3, 5, 10, 20, 50, 100], 'C': [0.001, 0.01, 0.1, 1, 5, 10, 20, 50, 100, 200, 300]},\n",
    "#            {'kernel': ['poly'], 'gamma': [0.001, 0.01, 0.1, 1, 2, 3, 5, 10, 20, 50, 100], 'C': [0.001, 0.01, 0.1, 1, 5, 10, 20, 50, 100, 200, 300], 'degree': [3, 5, 7, 10, 14, 21]}]\n",
    "params = [{'kernel': ['rbf'], 'gamma': [ 225, 250, 275, 300], 'C': [ 1, 2, 5, 7, 10, 13,]}]\n",
    "\n",
    "print \"Tuning SVM parameters\"\n",
    "clf = GridSearchCV(SVC(), params, cv=5,  scoring='accuracy')\n",
    "clf.fit(data, target)\n",
    "\n",
    "# print(\"Best parameters set found on development set:\")\n",
    "# print()\n",
    "# print(clf.best_params_)\n",
    "# print()\n",
    "# print(\"Grid scores on development set:\")\n",
    "# print()\n",
    "print \"best params: {}\".format(clf.best_params_)\n",
    "print \"all results:\"\n",
    "for params, mean_score, scores in clf.grid_scores_:\n",
    "    print \"{:5.3f} (+/- {:5.3f}) for {}\".format(mean_score, scores.std() * 2, params)\n",
    "#     print \"%0.3f (+/-%0.03f) for %r\" % (mean_score, scores.std() * 2, params)\n",
    "# print()\n",
    "\n",
    "# print(\"Detailed classification report:\")\n",
    "# print()\n",
    "# print(\"The model is trained on the full development set.\")\n",
    "# print(\"The scores are computed on the full evaluation set.\")\n",
    "# print()\n",
    "# y_true, y_pred = y_test, clf.predict(X_test)\n",
    "# print(classification_report(y_true, y_pred))\n",
    "# print()\n",
    "\n",
    "###############################################\n",
    "\n",
    "# scores = ['precision', 'recall']\n",
    "\n",
    "# for score in scores:\n",
    "#     print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "#     print()\n",
    "\n",
    "#     clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5,\n",
    "#                        scoring='%s_weighted' % score)\n",
    "#     clf.fit(X_train, y_train)\n",
    "\n",
    "#     print(\"Best parameters set found on development set:\")\n",
    "#     print()\n",
    "#     print(clf.best_params_)\n",
    "#     print()\n",
    "#     print(\"Grid scores on development set:\")\n",
    "#     print()\n",
    "#     for params, mean_score, scores in clf.grid_scores_:\n",
    "#         print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "#               % (mean_score, scores.std() * 2, params))\n",
    "#     print()\n",
    "\n",
    "#     print(\"Detailed classification report:\")\n",
    "#     print()\n",
    "#     print(\"The model is trained on the full development set.\")\n",
    "#     print(\"The scores are computed on the full evaluation set.\")\n",
    "#     print()\n",
    "#     y_true, y_pred = y_test, clf.predict(X_test)\n",
    "#     print(classification_report(y_true, y_pred))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score against validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20036934441366575"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(kernel='rbf', C=10, gamma=175)\n",
    "clf.fit(data, target)\n",
    "clf.score(data_val, target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
